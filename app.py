import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
import pandas as pd
import unicodedata
from concurrent.futures import ThreadPoolExecutor
from fuzzywuzzy import fuzz

# -----------------------------
# LOAD SECRETS
# -----------------------------
BASE_URL = st.secrets["TELEGRAM_BASE_URL"]
START_POST = int(st.secrets["START_POST"])
END_POST = int(st.secrets["END_POST"])
MAX_THREADS = 10  # concurrency level

# -----------------------------
# PAGE CONFIG & STYLING
# -----------------------------
st.set_page_config(page_title="ğŸ“Œ ×—×™×¤×•×© ×”×–×“×× ×•×™×•×ª ×’×™×•×¡", page_icon="ğŸ”", layout="wide")

st.markdown(
    """
    <style>
        body { direction: rtl; text-align: right; }
        .stTextInput > div > div > input { text-align: right; }
        .stDataFrame, .stTable { direction: rtl; }
        .stButton > button {
            font-size: 18px;
            font-weight: bold;
            background-color: #FF4B4B;
            color: white;
            width: 100%;
        }
    </style>
    """,
    unsafe_allow_html=True,
)

# -----------------------------
# ESCAPE THE 'â¬…ï¸' CHARACTER
# -----------------------------
arrow_escaped = re.escape("â¬…ï¸")

# -----------------------------
# NORMALIZE HEBREW (for fuzzy search)
# -----------------------------
def normalize_hebrew(text: str) -> str:
    """
    1) NFKC normalization
    2) Remove quotes (×´, ", ')
    """
    text = unicodedata.normalize('NFKC', text)
    for ch in ['×´', '"', "'"]:
        text = text.replace(ch, "")
    return text

# -----------------------------
# PARSE SERVICE PERIOD MONTHS
# -----------------------------
def parse_service_period(text: str) -> (str, str):
    """
    If text looks like "××¨×¥ - ××¤×¨×™×œ", return ("××¨×¥", "××¤×¨×™×œ").
    Otherwise ("", "").
    """
    text = text.strip()
    pattern = r"^\s*(\S+)\s*-\s*(\S+)\s*$"
    match = re.search(pattern, text)
    if match:
        return match.group(1), match.group(2)
    return "", ""

# -----------------------------
# PARSE P'TOR / MAAGAR
# -----------------------------
def parse_exempt_line(text: str):
    """
    Look for a line that starts with either â›” or ğŸ–ğŸ» and references ×¤×˜×•×¨ or ×××’×¨.
    We'll return two booleans: (relevant_ptor, relevant_maagar),
    which can be True/False or None if unknown.
    """
    # Regex for a line containing either 'â›”' or 'ğŸ–ğŸ»', plus '×¤×˜×•×¨' or '×××’×¨',
    # near the end, just before "×œ×¤×¨×˜×™× × ×•×¡×¤×™× ×•×”×’×©×ª ××•×¢××“×•×ª" or a dashed line.
    # We'll just find the FIRST occurrence with "â›”" or "ğŸ–ğŸ»" that mentions '×¤×˜×•×¨' or '×××’×¨'.
    pattern = r"[â›”ğŸ–ğŸ»].*?(?:×¤×˜×•×¨|×××’×¨).*"
    match = re.search(pattern, text)
    if not match:
        return (None, None)  # Not found => no info

    line = match.group(0)

    # Start by defaulting to None
    ptor = None
    maagar = None

    # ---- Check P'TOR ----
    # "×œ× ×¨×œ×•×•× ×˜×™ ×œ×‘×¢×œ×™ \"×¤×˜×•×¨\"" => ptor=False
    # "×¨×œ×•×•× ×˜×™ ×’× ×œ×‘×¢×œ×™ \"×¤×˜×•×¨\"" => ptor=True
    # "×œ× ×¤×˜×•×¨" => ptor=False
    if "×œ× ×¨×œ×•×•× ×˜×™ ×œ×‘×¢×œ×™ \"×¤×˜×•×¨\"" in line:
        ptor = False
    elif "×œ×‘×¢×œ×™ \"×¤×˜×•×¨\"" in line:
        # e.g. "×¨×œ×•×•× ×˜×™ ×’× ×œ×‘×¢×œ×™ \"×¤×˜×•×¨\"" or "××ª××™× ×œ×‘×¢×œ×™ \"×¤×˜×•×¨\""
        ptor = True
    if "(×œ× ×¤×˜×•×¨!)" in line:
        # explicitly says not ptor
        ptor = False

    # ---- Check MA'AGAR ----
    # "×œ× ×¨×œ×•×•× ×˜×™ ×œ××©×•×™×™×›×™× ×œ\"×××’×¨\"" => maagar=False
    # "××©×•×™×™×›×™× ×œ\"×××’×¨\"" => maagar=True
    # or if line includes "×œ××©×•×™×™×›×™× ×œ\"×××’×¨\" (×œ× ×¤×˜×•×¨!)" => maagar=True, ptor=False
    if "×œ× ×¨×œ×•×•× ×˜×™" in line and "××©×•×™×™×›×™× ×œ\"×××’×¨\"" in line:
        maagar = False
    elif "××©×•×™×™×›×™× ×œ\"×××’×¨\"" in line:
        # e.g. "×¨×œ×•×•× ×˜×™ ×’× ×œ×‘×¢×œ×™ \"×¤×˜×•×¨\" / ××©×•×™×™×›×™× ×œ\"×××’×¨\""
        if maagar is None:
            maagar = True

    return (ptor, maagar)

# -----------------------------
# REGEX PARSING
# -----------------------------
def parse_ad_number(text: str) -> str:
    match = re.search(r"××•×“×¢×”\s*××¡×¤×¨\s*#(\d+)", text)
    return match.group(1) if match else "×œ× × ××¦×"

def parse_between(text: str, start_marker: str) -> str:
    pattern = rf"{start_marker}\s*:\s*([\s\S]*?)(?=\n-+\s|\n{arrow_escaped}|$)"
    m = re.search(pattern, text)
    return m.group(1).strip() if m else ""

def parse_section(text: str, section_title: str) -> str:
    pattern = rf"{arrow_escaped}\s*{section_title}\s*:\s*([\s\S]*?)(?=\n{arrow_escaped}|\n-+\s|$)"
    m = re.search(pattern, text)
    if not m:
        return ""
    extracted = re.sub(r"-+\s*", "", m.group(1)).strip()
    return extracted

def parse_roles(text: str) -> list:
    pattern = rf"{arrow_escaped}\s*×“×¨×•×©×™×\s*:\s*([\s\S]*?)(?=\n{arrow_escaped}|\n-+\s|$)"
    m = re.search(pattern, text)
    if not m:
        return []
    roles_section = m.group(1)
    roles_list = re.findall(r"\*\*\s*(.+)", roles_section)
    return [r.strip() for r in roles_list]

# -----------------------------
# parse_job_info
# -----------------------------
def parse_job_info(post_id: int, html_content: str):
    if not html_content:
        return None

    soup = BeautifulSoup(html_content, "html.parser")
    meta_desc = soup.find("meta", {"property": "og:description"})
    if not meta_desc:
        return None

    text_content = meta_desc["content"]

    # 1) Must have ××•×“×¢×” ××¡×¤×¨ #XXXX
    ad_number = parse_ad_number(text_content)
    if ad_number == "×œ× × ××¦×":
        return None  # skip if no ad number

    # 2) Extract fields
    sug_yehida = parse_between(text_content, "×¡×•×’ ×™×—×™×“×”")
    area = parse_between(text_content, "××–×•×¨ ×‘××¨×¥")
    roles = parse_roles(text_content)
    qualifications = parse_section(text_content, "×›×™×©×•×¨×™× × ×“×¨×©×™×")
    unit_info = parse_section(text_content, "×¤×¨×˜×™× ×¢×œ ×”×™×—×™×“×”")
    service_terms = parse_section(text_content, "×ª× ××™ ×©×™×¨×•×ª")

    # 3) Remove trailing "××–×•×¨ ×‘××¨×¥" from sug_yehida if present
    if "××–×•×¨ ×‘××¨×¥:" in sug_yehida:
        sug_yehida = sug_yehida.split("××–×•×¨ ×‘××¨×¥:")[0].strip()

    # 4) Service period
    service_period_raw = parse_between(text_content, "×ª×§×•×¤×ª ×©×™×¨×•×ª ×”×§×¨×•×‘×”")
    month_start, month_end = parse_service_period(service_period_raw)

    # 5) Immediate, recruitment type
    immediate = "×›×Ÿ" if "â°" in text_content else "×œ×"
    recruitment_type = "×–×× ×™ ××• ×§×‘×•×¢" if "ğŸ”Š ×–×× ×™ ××• ×§×‘×•×¢" in text_content else ""

    # 6) Parse p'tor / maagar
    ptor_bool, maagar_bool = parse_exempt_line(text_content)
    # We'll store them as strings "×›×Ÿ" / "×œ×" / "" for easy filtering
    if ptor_bool is True:
        ptor_str = "×›×Ÿ"
    elif ptor_bool is False:
        ptor_str = "×œ×"
    else:
        ptor_str = ""  # unknown

    if maagar_bool is True:
        maagar_str = "×›×Ÿ"
    elif maagar_bool is False:
        maagar_str = "×œ×"
    else:
        maagar_str = ""  # unknown

    if not roles:
        roles = ["×œ× ×¦×•×™× ×• ×ª×¤×§×™×“×™×"]

    results = []
    for role in roles:
        row = {
            "××¡×¤×¨ ××•×“×¢×”": ad_number,
            "×ª×¤×§×™×“": role,
            "×¡×•×’ ×™×—×™×“×”": sug_yehida,
            "××–×•×¨ ×‘××¨×¥": area,
            "×›×™×©×•×¨×™× × ×“×¨×©×™×": qualifications,
            "×¤×¨×˜×™× ×¢×œ ×”×™×—×™×“×”": unit_info,
            "×ª× ××™ ×©×™×¨×•×ª": service_terms,
            "×ª×§×•×¤×ª ×©×™×¨×•×ª (Raw)": service_period_raw,
            "×—×•×“×© ×”×ª×—×œ×”": month_start,
            "×—×•×“×© ×¡×™×•×": month_end,
            "×’×™×•×¡ ××™×™×“×™": immediate,
            "×¡×•×’ ×’×™×•×¡": recruitment_type,
            # new fields for ×¤×˜×•×¨ / ×××’×¨
            "××ª××™× ×œ×‘×¢×œ×™ ×¤×˜×•×¨": ptor_str,     
            "××ª××™× ×œ××©×•×™×™×›×™× ×œ×××’×¨": maagar_str,
            "×§×™×©×•×¨": f"{BASE_URL}{post_id}"
        }
        results.append(row)
    return results

# -----------------------------
# DOWNLOAD HTML
# -----------------------------
def download_html(post_id: int):
    url = f"{BASE_URL}{post_id}"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        resp = requests.get(url, headers=headers, timeout=5)
        if resp.status_code == 200:
            return post_id, resp.text
    except:
        pass
    return post_id, None

# -----------------------------
# SCRAPE (Multithread)
# -----------------------------
@st.cache_data
def scrape_jobs_concurrent(start_id: int, end_id: int) -> pd.DataFrame:
    data = []
    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        html_results = list(executor.map(download_html, range(start_id, end_id+1)))

    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:
        parsed_lists = list(executor.map(lambda x: parse_job_info(x[0], x[1]), html_results))

    for plist in parsed_lists:
        if plist:
            data.extend(plist)

    return pd.DataFrame(data)

# -----------------------------
# FUZZY MATCH HELPER
# -----------------------------
def fuzzy_score_row(row, query):
    row_text = " ".join(str(v) for v in row.values)
    row_text = normalize_hebrew(row_text)
    query = normalize_hebrew(query)
    return fuzz.partial_ratio(query.lower(), row_text.lower())

# -----------------------------
# MAIN APP
# -----------------------------
st.title("ğŸ“Œ ×—×™×¤×•×© ×”×–×“×× ×•×™×•×ª ×’×™×•×¡")

with st.spinner("ğŸ”„ ×˜×•×¢×Ÿ ××•×“×¢×•×ª..."):
    df = scrape_jobs_concurrent(START_POST, END_POST)

st.success("âœ… ×›×œ ×”××•×“×¢×•×ª × ×˜×¢× ×• ×‘×”×¦×œ×—×”!")

st.header("×¡×™× ×•×Ÿ ×•×—×™×¤×•×©")

search_query = st.text_input("ğŸ” ×—×™×¤×•×© ×—×•×¤×©×™ (×‘×›×œ ×”×©×“×•×ª):", "")

# existing filters
all_areas = ["(×”×›×œ)"] + sorted(set(df["××–×•×¨ ×‘××¨×¥"].dropna()))
selected_area = st.selectbox("×¡×™× ×•×Ÿ ×œ×¤×™ ××–×•×¨ ×‘××¨×¥:", all_areas, index=0)

all_units = ["(×”×›×œ)"] + sorted(set(df["×¡×•×’ ×™×—×™×“×”"].dropna()))
selected_unit = st.selectbox("×¡×™× ×•×Ÿ ×œ×¤×™ ×¡×•×’ ×™×—×™×“×”:", all_units, index=0)

# month filters
all_start_months = ["(×”×›×œ)"] + sorted(set(df["×—×•×“×© ×”×ª×—×œ×”"].dropna()))
selected_month_start = st.selectbox("×¡×™× ×•×Ÿ ×œ×¤×™ ×—×•×“×© ×”×ª×—×œ×”:", all_start_months, index=0)

all_end_months = ["(×”×›×œ)"] + sorted(set(df["×—×•×“×© ×¡×™×•×"].dropna()))
selected_month_end = st.selectbox("×¡×™× ×•×Ÿ ×œ×¤×™ ×—×•×“×© ×¡×™×•×:", all_end_months, index=0)

# new filters for ×¤×˜×•×¨ and ×××’×¨
all_ptor = ["(×”×›×œ)", "××ª××™× ×œ×‘×¢×œ×™ ×¤×˜×•×¨", "×œ× ××ª××™× ×œ×‘×¢×œ×™ ×¤×˜×•×¨"]
selected_ptor = st.selectbox("×¡×™× ×•×Ÿ ×œ×¤×™ ×¤×˜×•×¨:", all_ptor, index=0)

all_maagar = ["(×”×›×œ)", "××ª××™× ×œ××©×•×™×™×›×™× ×œ×××’×¨", "×œ× ××ª××™× ×œ××©×•×™×™×›×™× ×œ×××’×¨"]
selected_maagar = st.selectbox("×¡×™× ×•×Ÿ ×œ×¤×™ ×××’×¨:", all_maagar, index=0)

# check if user applied any filter or typed search
filters_used = (
    search_query.strip() != "" or
    selected_area != "(×”×›×œ)" or
    selected_unit != "(×”×›×œ)" or
    selected_month_start != "(×”×›×œ)" or
    selected_month_end != "(×”×›×œ)" or
    selected_ptor != "(×”×›×œ)" or
    selected_maagar != "(×”×›×œ)"
)

if not filters_used:
    st.info("×× × ×”×–×Ÿ ×—×™×¤×•×© ××• ×”×’×“×¨ ×¡×™× ×•×Ÿ ×›×“×™ ×œ×¨××•×ª ×ª×•×¦××•×ª.")
    st.stop()

# -----------------------------
# Apply filters
# -----------------------------
filtered_df = df.copy()

# 1) Fuzzy search
if search_query.strip():
    threshold = 70
    scores = filtered_df.apply(lambda r: fuzzy_score_row(r, search_query), axis=1)
    filtered_df = filtered_df[scores >= threshold]

# 2) Dropdown filters
if selected_area != "(×”×›×œ)":
    filtered_df = filtered_df[filtered_df["××–×•×¨ ×‘××¨×¥"] == selected_area]

if selected_unit != "(×”×›×œ)":
    filtered_df = filtered_df[filtered_df["×¡×•×’ ×™×—×™×“×”"] == selected_unit]

if selected_month_start != "(×”×›×œ)":
    filtered_df = filtered_df[filtered_df["×—×•×“×© ×”×ª×—×œ×”"] == selected_month_start]

if selected_month_end != "(×”×›×œ)":
    filtered_df = filtered_df[filtered_df["×—×•×“×© ×¡×™×•×"] == selected_month_end]

# Filter by ×¤×˜×•×¨
if selected_ptor == "××ª××™× ×œ×‘×¢×œ×™ ×¤×˜×•×¨":
    filtered_df = filtered_df[filtered_df["××ª××™× ×œ×‘×¢×œ×™ ×¤×˜×•×¨"] == "×›×Ÿ"]
elif selected_ptor == "×œ× ××ª××™× ×œ×‘×¢×œ×™ ×¤×˜×•×¨":
    filtered_df = filtered_df[filtered_df["××ª××™× ×œ×‘×¢×œ×™ ×¤×˜×•×¨"] == "×œ×"]

# Filter by ×××’×¨
if selected_maagar == "××ª××™× ×œ××©×•×™×™×›×™× ×œ×××’×¨":
    filtered_df = filtered_df[filtered_df["××ª××™× ×œ××©×•×™×™×›×™× ×œ×××’×¨"] == "×›×Ÿ"]
elif selected_maagar == "×œ× ××ª××™× ×œ××©×•×™×™×›×™× ×œ×××’×¨":
    filtered_df = filtered_df[filtered_df["××ª××™× ×œ××©×•×™×™×›×™× ×œ×××’×¨"] == "×œ×"]

# -----------------------------
# Show results
# -----------------------------
st.write(f"× ××¦××• {len(filtered_df)} ×ª×•×¦××•×ª:")

if len(filtered_df) == 0:
    st.warning("×œ× × ××¦××• ×ª×¤×§×™×“×™× ×‘××¢×¨×›×ª ×”×ª×•×××™× ×œ×—×™×¤×•×© / ×¡×™× ×•×Ÿ ×©×œ×š.")
else:
    for idx, row in filtered_df.iterrows():
        ad_number = row["××¡×¤×¨ ××•×“×¢×”"]
        role = row["×ª×¤×§×™×“"]
        link = row["×§×™×©×•×¨"]
        st.markdown(f"- **{role}** (××•×“×¢×” #{ad_number}): [×§×™×©×•×¨ ×œ×¤×¨×˜×™×]({link})")
